#labels Phase-Requirements
= Introduction =

The Lookup Service is a critical piece of the perfSONAR infrastructure, and it is important to verify that
the performance scales in all dimensions, including number of clients, number of hLS's, and amount of data being registered in each hLS. 

This page describes a test plan to be performed on the ESnet OpenDevNet. OpenDevNet will allow mutiple VMs to be used to simulate a reasonably large test environment.

= LS Testing Issues =

One must be careful to create a separate gLS/hLS hierarchy for testing, so as to not pollute the global production hLS with bogus data. For more information, see: [http://code.google.com/p/perfsonar-ps/wiki/LocalLSDeployment].
The service emulator [http://anonsvn.internet2.edu/svn/perfSONAR-PS/trunk/client/fakeService/ FakeService] can be used to register data into the hLS. 

For all testing, the following should be monitored and recorded:
 * correctness: are the results correct
 * performance: how long does a client request take
 * load: what is the CPU load on the server
 * DB errors: at what point does xmldb start giving errors (Q: How many errors are acceptable? )

Another issue is the maximum number of VMs supported by OpenDevNet. This is currently around 15, and will be around 100 in a few months. It should be possible to run multiple instances of client.pl, fakeService.pl, and ls_registration_daemon.pl from the same VM. It is NOT advisable to run multiple hLS or gLS in the same VM. (Q: what about doing 1 hLS and 1 gLS?)

== Basic hLS correctness Testing ==

 * register fake services in an hLS, and then write a client to query the gLS for the service, and make sure the results are the same as what was registered. 
 * Q: Are there different types of fake services that should be tested independently? If so, please add details here.

Determine the amount of time for each of the following:

== hLS scalability: number of services registered ==

 * What is the maximum number of services that can be registered by a single instance of `ls_registration_daemon`?
 * What is the maximum number of 'fake_services' instances can be run to a single hLS?

== LS scalability: number of clients ==
 * how many simultaneous clients can contact a single gLS asking for data from the same hLS?
 * how many simultaneous clients can contact a single gLS asking for data from the different hLSs?

== gLS scalability: number of hLSs ==
 * what is the maximum number of hLSs that can register to a single gLS?

== Combined testing ==
 * what is the limit on an hLS and the number of both clients and service registrations increases?
  ** this will test if doing both reading and writing to xmldb is worse that just reading or just writing 

== Failover Testing ==
 * test the performance for a client to detect a failed server and try the backup server instead.

=== Test Assumptions  ===
 * each MA registers with 2 hLS's for redundancy
 * all services set ls_registration_interval to 120 (Q: or should this be shorter?)
 * the synchronization rate should tested at the following intervals: 
   ** 30,60,120 min updates (Q: is 30 min short enough?)
   ** these should be tested for both service to hLS updates and hLS to gLS, and gLS to gLS synchronization rates

== Notes ==
Complete testing must involve the ability to modify each of the following independently:
 * the gLS to gLS synchronization update rate
 * the hLS to gLS synchronization update rate
 * the service to hLS update rate
 * the number of servces
 * the number of gLS's
 * the number of (src/dest/eventTypes) that a service registers

The following should be measured for each test configuration:
 * Query time for a single requests
 * Query time for simultaneous requests (5, 10, 50) to a single gLS
 * measure the change in query time when a hLS or gLS fails