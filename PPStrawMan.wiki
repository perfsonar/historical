= Overall Development Plan !StrawMan =

<wiki:comment>Alas, even though google documents it, this toc doesn't seem to work.</wiki:comment>
<wiki:toc max_depth="2" />

 * [PPStrawMan#Motivation Motivation]
 * [PPStrawMan#High_Level_Break-down_of_Priorities High Level Break-down of Priorities]
   * [PPStrawMan#Network_Administrator_Interfaces_to_Dynamic-Hybrid_Networks Network Administrator Interfaces to Dynamic-Hybrid Networks]
   * [PPStrawMan#End-user_and_Application_Interfaces_to_Dynamic-Hybrid_Networks End-user and Application Interfaces to Dynamic-Hybrid Networks]
   * [PPStrawMan#Research_Next-Generation_Networks Research Next-Generation Networks]
 * [PPStrawMan#Project_Breakdown Project Breakdown]
   * [PPStrawMan#Dynamic_Circuits Dynamic Circuits]
     * [PPStrawMan#0.5_Architecture_Enhancements 0.5 Architecture Enhancements]
     * [PPStrawMan#0.5+_components 0.5+ components]
     * [PPStrawMan#0.6_Architecture_Enhancements 0.6 Architecture Enhancements]
     * [PPStrawMan#DCN_Additional_Desired_Enhancements Additional Desired Enhancements]
   * [PPStrawMan#Performance_Monitoring_(perfSONAR) Performance Monitoring (perfSONAR)]
     * [PPStrawMan#pS-PS_3.1_Enhancements pS-PS 3.1 Enhancements]
     * [PPStrawMan#pS-PS_3.1+_enhancements pS-PS 3.1+ enhancements]
     * [PPStrawMan#pS-NPToolkit_3.1_(last_knoppix_version) pS-NPToolkit 3.1 (last knoppix version)]
     * [PPStrawMan#pS-PS_3.2_enhancements pS-PS 3.2 enhancements]
     * [PPStrawMan#pS-NPToolkit_3.2 pS-NPToolkit 3.2]
     * [PPStrawMan#pS-PS_4.0 pS-PS 4.0]
     * [PPStrawMan#pS-NPDomain_4.0 pS-NPDomain 4.0]
     * [PPStrawMan#pS_Additional_Desired_Enhancements Additional Desired Enhancements]

== Motivation ==

The evolution of computer network technology is quickly transitioning from an environment of packet networks built ontop of stable circuit networks to a much more dynamic hybrid model. (See GENI, DCN, !OpenFlow, etc...) The new paradigms will require a different interaction model. The clean model of administrators completely handling traffic engineering issues is going away in support of a model where end users/applications are able to exert more direct influence. This is seen even at the application layer with P2P efforts.

This shift represents both a challenge and an opportunity for Internet2.  For Internet2 to thrive, it must be a leader in this paradigm shift.  This shift provides an opportunity for the Internet2 community to influence the shape of future networking. The challenge is to stay at the front of this paradigm shift without dropping too much of the diverse community off the back.

As both a back-bone provider and a community for U.S. R&E networks, Internet2 must focus its efforts on things that satisfy three primary stakeholders: the community of R&E network providers including peers, connectors, regionals and end site universities/labs/organizations; the community of network users including students, researchers, faculty and staff; and the community of network researchers developing the next evolution of network technology.

== High Level Break-down of Priorities ==

Fundamentally, different control structures will be required to make this paradigm shift work. End users will need to be able to make more targetted requests for network resources. Much of the traffic engineering that was once the providence of network administrators needs to be automated and made available to end users/applications/peer networks. Of course, network administrators will need to retain enough control to protect the resources as well as to diagnose problems.

In addition to projects to support these two high-level more network usage interested constituents, it is critical for Internet2's future vitality for it to participate in some less immediate focused research. This is not only to support the research community, but represents the only way Internet2 can reasonably evaluate new technologies as they become available.

The following is a break-down of the areas Internet2 should engage in from a top-down priorities perspective (specific projects with resource/milestones to follow):

=== Network Administrator Interfaces to Dynamic-Hybrid Networks ===
  * Traditional IP routing
  * DCN-SS
    # Quickly configure/allocate scheduled circuits
  * perfSONAR
    # Share network diagnostic information across administrative boundries
  * Traditional NOC tools
    * Measurement/Monitoring tools
    * Integration of tools such as nagios/ticketing/managment with federated models of AA and data sharing (pS).
  * Administrator Interfaces
    * Web-based GUIs
      * Integrate new control and monitoring interfaces into administrator dash-boards. (Including capacity planning.)
      * Some focused on the backbone provider, some focused from the connector or even peer network perspective.
    * Automated administrator interfaces. (e.g. flow-data initiats action)
  
=== End-user and Application Interfaces to Dynamic-Hybrid Networks ===
  * Traditional IP routing
  * Phoebus
    * Router-gateways to circuits
    * Add intellegence to the network at the session layer
  * User/Application Interfaces
    * Web-based GUIs
      * Focused on specific user communities (i.e. LHC, LIGO etc...)
      * Allow end-users to specifically provision end-to-end packet networks
      * Allow end-users to determine specifically what infrastructure their traffic traverses and diagnose performance problems.
    * Automated interfaces
      * Allow an application to signal the need for more/less resources
      * Allow an application to receive performance data so it can make decisions on how to utilize the available resources

=== Research Next-Generation Networks ===
  * GENI
  * !OpenFlow

== Project Breakdown ==

The above priorities can be viewed as features desired from an integrated (but distributed) network system. The pieces of that system along with the current priorities follows:

=== Dynamic Circuits ===

The primary short-term goals for the DCN-SS is to modularize the functionality. This is needed to allow the architecture to survive more interations of network technology evolution. (If the abstraction is sound, specific modules can be updated in support of new evolutions without all modules needing to be updated.)

==== 0.5 Architecture Enhancements ====

  * Automatic translatio of OSPF-TE to populate topology service 
  * Modular AAA (RMI interface)
  * Notification Broker
  * Accounting 'hooks'

==== 0.5+ components ====

Not as part of any full DCN-SS release, the following components will need to be developed and released in a limited fashion in support of very specific IDCs. (In support of the Internet2 pilot service and/or specific constituents such as LHC.)

  * Accounting service
    * In accordance with the buisiness case decided upon by the DCN WG
  * VO based authorization
    * Priority to LHC circuits over USLHCNET

==== 0.6 Architecture Enhancements ====

  * Web-servcies interfaces to all modules (To support cross-project reuse.)
  * Modular path-computation
  * Modular path setup and teardown
  * Integration with perfSONAR for circuit monitoring

=== DCN Schedule ===

These dates are very agressive, but do assume approximately the current resources. (With a guess that some small fraction of additional funding will come through from grant submissions.)

|| Date || Milestone ||
|| March || Release of 0.5 ||
|| May || Prototype Accounting Service ||
|| May || Limited VO authorization case (USLHCNET) ||
|| June || Kick-off of Pilot Service ||
|| October || Release of 0.6 ||

==== DCN Additional Desired Enhancements ====

  * Higher level of phoebus/DCN integration. (IDC controlling pho-gateways - treat them as a L4 switch)
    # 1 yr, 1 FTE
  * DICE-AA integrated AAA supported
    # 2 yr, 2 FTE
  * Extend control-plane down into Layer-1 (VNC)
    * Multi-dimensional path finding (VNC)
    * layers, authorizations, schedules
    # 3 yr, 2 FTE
  * Modular scheduling component
    * Allow application scheduling as well (bwctl, video-conference, etc...)
    # 2 yr, 1 FTE
  * performance-data triggered automated circuit scheduling (FAATE)
    # 3 yr, 3 FTE
  * Unified control-plane API
    * TP/LS/Pho API merge
    * Model for how user-level networks can be overlaid
    # 3 yr, 3 FTE
  * IDC integration with OpenFlow
  * Develop topological querying interfaces for 'what-if' scenerios.
    * go around heavily utilized shortest path problems
    # 2 yr, 2 FTE
  * Develop methods for automated xml-topology generation for intra-domain as well as automated 'abstract topology' generation from that for inter-domain exchanges
    # 1 yr, 2 FTE
  * Develop Point-to-Multipoint and Mutipoint-to-Multipoint services
    # ?
  * Develop mechanisms for ethernet QinQ services on DCN
    # ?
  * Develop enhanced GUI or script based config/installation tools for DCN-SS
    # 2 yr, 2 FTE
  * Develop GUI/Map point-n-click based provisioning system
    # 1 yr, 2 FTE

=== Performance Monitoring (perfSONAR) ===

The primary short-term goals for perfSONAR development is: to harden and make useful GUIs for existing backbone, regional, and community related deployments (i.e. the LHC community), enable diagnostics of dynamic circuits, and to add the features needed to entice additional deployments into regionals, campuses, and labs.

==== pS-PS 3.1 Enhancements ====

  * OWAMP support in perfSONAR-BUOY
  * gLS/hLS packages
  * gLS/hLS performance and stability updates
  * CGI based GUIs
    * gLS/hLS GUIs
    * simple data GUIs for owamp/bwctl/pingER/SNMP
  * Circuit Monitoring (glif version, still E2EMon compatible)
  * Additions to pS-NPToolkit to enable Zero-Config mode
  * yum/apt packaging

==== pS-PS 3.1+ enhancements ====

In support of the CTP project (*if applicable*). Otherwise, some of these things will be postponed and/or cancelled.

  * Nagios pluggins for threshold alarms on performance data
  * Enhanced CGI script for owamp data (dash-board)
  * Modified pS-B owamp database back-end with support for history of delay-variation

==== pS-NPToolkit 3.1 (last knoppix version) ====

  * Use new versions of owamp/bwctl and pS-PS 3.1(+)
  * Minimal-configuration start-up for diagnostic
  * All Administration via HTTP
  * Stability and performance enhancements
  * Community specific operations

==== pS-PS 3.2 enhancements ====

  * DCN triggered link monitoring
  * limited AA integration

==== pS-NPToolkit 3.2 ====

  * Fedora-Live based (drop knoppix) to enable ISO building from rpms
  * Use pS-PS 3.2

==== pS-PS 4.0 ====

  * LS - Topology based service queries (nearest MP problem)
  * Centralized Configuration Service
  * GUI enhancements
    * User community centric - search for problems relating to X
  
==== pS-NPDomain 4.0 ====

  * Allow an administrator to control a set of pS-performance nodes
  * Combination of yum repository and centralized configuration management tools

=== pS Schedule ===

These dates are very aggressive, but do assume approximately the current resources. (With a guess that some small fraction of additional funding will come through from grant submissions.)

|| Date || Milestone ||
|| 27 March || pS-PS 3.1 ||
|| 27 April || pS-PS 3.1+ ||
|| May || pS-NPToolkit 3.1 ||
|| June || pS-PS 3.2 ||
|| July || pS-NPToolkit 3.2 ||
|| Jan 10 || pS-PS 4.0 ||
|| Jan 10 || pS-NPDomain 4.0 ||

==== pS Additional Desired Enhancements ====

  * Integration into end applications (SyNAPSE)
    # 3 yr, 2 FTE
  * pS-Netflow - archive of netflow data
    * allow queries to search for specific patterns
    * allow queries for specific summary stats (what ports used, gross traffic flows, IPv4 vs IPv6 etc...)
    # 0.5 yr, 1 FTE.
  * BGP archive
    * What would the route have been for a packet at time X at location Y be?
    # 1 yr, 1 FTE.
  * Traceroute MA
    # 0.5 yr, 0.5 FTE.
  * More NOC integration
    * Tickets-MA - tag a ticket as relating to specific topology and correlate with performance data
    * Needs to include GUI work for both TT integration, and data usage. For example, correlating a reported maintenance window could flag performance data as suspect.
    # 3 yrs, 2 FTE.
    # Dependent on integration to NOC TT GUIs and Topology services
  * Global - state of the networks GUIs. Provide ways of seeing an overall picture of perfSONAR deployments and then allowing the user to drill down to specific regions (both geographically, as well as by data types or network technology types - or even application communities). Basically, we need something better than pS-UI as a 'general' user interface. (This does not take the place of the dashboards required for specific application-communities or specific regional deployments but is needed to show the scope of the effort. For example, to sell the ideas to others.)
    # 2 yr, 2 FTE.
  * perfSONAR interfaces for sharing of ProtoGENI performance and management data. (LAMP)
    # 3 yr, 1 FTE.
  * DICE-AA integrated AAA supported
    # 2 yr, 2 FTE

=== Phoebus ===


The primary short-term goals for phoebus development is: protocol separation of data and control, performance enhancements, and stability and usability enhancements.

==== Phoebus 1.1 Enhancements ====

  * Split of data/control protocols at the gateways
  * Use pS-gLS infrastructure to find available gateways
  * Session signaling
  * Session level framing
  * Modifications to support 0.5 DCN

==== Phoebus 1G Pilot Deployment ====

  * 1G gateways at all Internet2 backbone router POPs
  * 1G DCN connections available at each location

==== Phoebus 2.0 Enhancements ====

  * Manage several gateways from a single control-point (IDC integration)
  * Use Information Services (Path-Finding) services to compute paths (depends on phoebus-gateways being registered along with topology)
  * Support O(10G) flows
  * Integrate in service monitoring features so multiple gateways can be managed more effectively

=== Phoebus Schedule ===

This schedule assumes that most of the development work is happening at UDel and that Internet2 resources for the UDel contract(s) are relatively stable.

|| Date || Milestone ||
|| June || Phoebus 1.1 ||
|| July || Phoebus 1G Pilot ||
|| February 10 || Phoebus 2.0 ||

==== Phoebus Additional Desired Enhancements ====

  * Native windows client support
    # 0.5 yr, 0.5 FTE
  * Native MacOS client support
    # 0.5 yr, 0.5 FTE

== Last Updated ==

$Id$