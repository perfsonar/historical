#summary Developers Meeting at Winter 2010 Joint Techs - Salt Lake City, UT

=  Jan 31 2010 Meeting =

== Where ==

Collegiate room (2nd Floor - right next to the main ballroom) in the [http://www.union.utah.edu A. Ray Olpin University Union].  

== When ==

January 31, 2010.
1:00pm - 5:00pm MDT

== Agenda/Minutes ==

 # Attendees: 
   * TBD
 # LiveCD Topics (*Jason*, *Aaron*)
   * Remaining 3.1.X releases
     * Targeting 2 - 3 more this year.  May need to worry about some next year depending on support...
       * 3.1.3 in April (pre MM)
       * 3.1.4 in July (pre JTs)
       * 3.1.5 in Nov/Dec (if needed - hard with SC)
     * Critical bug fixes and software upgrades only 
     * Will release faster if something *very* critical comes up
     * [http://lists.debian.org/debian-security-announce/2010/msg00010.html Debian security support for v. 4.0 (etch) is being EOL'ed on Feb 15th]
       * Upgrade?
       * Manually backport?
   * Support Structure
     * Need explicit language in a public area
     * Ex:
       * _We monitor X, Y, and Z for vulnderabilities_
       * _We are will support 3.1 releases through X date_
       * _We will make announcements on vulnerabilties within X days_
       * _We will have fixes available for vulnerabilities within X days_
     * SLAC Kernel Discussion
       * SLAC's policy: Like to support kernel versions tied to RHEL (e.g. we use 2.6.27, they like 2.6.31(32?))
       * Our (_unstated right now_) policy: Choose the [http://lkml.org/lkml/2008/10/11/235 long term support kernel] and follow the releases here.  
       * No one has requested we make a switch, but there have been questions if we would consider it (and if so - soon)
       * SLAC security alluded to being ok with 2.6.27 - as long as we publicize our reasons and promises of support
       * Setting a policy matters to the VOs (e.g. USATLAS).  Choosing a specific policy matters more to individual stake holders. We should weigh the options of following a packaged kernel linage vs what the support structure of the OSS community has to offer.  
       * We do need to talk about kernel versions going forward in 3.1.x and 3.2
         * Do we stick with a vanilla support kernel?
         * Do we follow what the distribution (e.g. RHEL/CentOS/Scientific) is using - N.B. that applying web100 to a vendor kernel is not pleasant
         * bye bye web100?  longer term perhaps - hard to convert npad/ndt to not use it. 
   * RHEL (3.2) Transition
     * Targeting Summer for 3.2 Betas
     * Must support 3.1 in parallel
     * VOs may be slow to adopt 3.2 - they are still not recommending 3.1 to Tier3s for example.  
     * Features to worry about
       * Wizard interfaces
       * Backend magic to manage the disk
       * Upgrade path?
       * RPMs for all services and packages
       * Kernels (see above) - and if we want to support web100 long term
       * Install to disk option
 # Nagios integration (*Jason*, *Brian*)
   * Worth experimenting with basic installation/configuration of NAGIOS on current generation?
   * Configuration
     * Static - easy stuff
       * Certain processes/data sets should always be monitored
         * httpd
         * ntpd (both is it up and is time in sync)
         * Disk levels - go below a threshold
         * Load Levels - go above a threshold
         * Process Count?
           * Total goes above some threshold
           * Too many of one type, e.g. too many owampd's or bwctld's (indicates people testing to you)
       * Alert intervals (how often to send alerts for the same outstanding event)
     * Dynamic - the hard stuff
       * Email addresse(s) to send alerts to (GUI tie in)
       * Proccesses/data to monitor 
         * ssh (if enabled)
         * owampd/bwctld/npad/ndt/pS daemons/pow(master|collector)/bw(master|collector) (if enabled)
         * pSB MA, SNMP MA, topology services, LS (if enabled)
            * not just is it running, but does it respond to queries    
         * mysqld (if enabled)      
         * data sets from mysql (owamp/bwctl/pinger where applicable)
           * if data is above/below (too many errors or too little bandwidth) a performance threshold
               * e.g.: last 3 bwctl tests for a given host pair below 100 Mbps
           * if data is older than a time threshold
              * e.g.: no new bwctl data added to the MA for host pair x,y in the last 24 hours
         * data sets from rrd (where applicable)
           * if data is above/below a performance threshold (too many errors, too much or too little utilization)
           * if data is older than a time threshold
     * GUI extensions to watch other instances of the toolkit in the wild [requires nrpe probably] (future enhancement)
     * Send a 'come online' alert to a pS address?
     * Decide which of the many Nagios GUIs to integrate (e.g.: http://www.debianhelp.co.uk/nagiosweb.htm)
 # Testing Platforms (*Eric*, *Jason*, *Brian*)
   * Eric: 
     # Define Supported Platforms
       * hardware archictecture + operating system/distribution + middleware.
       * try to support most of currently deployed plaftom
       * keep it to a minimum number
       * have at least one platform current (hardware/OS)
     # Create VM templates that matches the supported platforms
       * use a VM to build and package release. This allows reproductable build process
       * Duplicate VM from template and instantiate for testing (always starting from a known state
     # Create VM to run stub services for testing
       * needed for unit testing (automation)
       * needed for debuging and development
       * allows to model various configuration
     # The entire build and testing environment as a download.
       * This allows anyone to reproduce it anywhere.
       * Helps for automation and provisioning
       * Allows for collaborative debug (one may send a VM to another).
     # Create enough VMs for scalability testing. Some examples include
        * ability to test N clients all hitting the same service at the same time
        * ability to run the LS scalability tests described here:
           * http://code.google.com/p/perfsonar-ps/wiki/ProposedLStestPlan
     # Ability to run interoperability tests with perfSONAR MDM release
     # Ability to automate much of this to run tests before a release (longer term)
     # Available collaborative resources
       * OpenDevNet
       * NMI
       * Ad-hoc
   * Jason: 
     * Group decission - what are we supporting for platform and arch?
       * Post policy to various web sites (psps.internet2, software.internet2, others?)
       * re-tool the yum repo (e.g. have a centos 5 vs a centos 4 vs a scientific, etc.) for new packages
       * list will need to flow with the what the vendors and major science VOs are using
       * list also related to our availability to create/test
     * Packages
       * RPM build info in svn currently
       * Deb packages?
       * Each developer responsbile for mainitaing the spec file
       * Release team should be handling the final build and test (currently each developer does this)
       * Release team will handle the uploading and maintenance of the yum repo
     * Buidling and Testing
       * [https://nmi.cs.wisc.edu/ | NMI] - worth using?
       * VM farm 
         * Goal: create scripts that will build/test installation/test use on all arch and platforms in a single swoop
         * Need VMs for each arch and platform, preferably 'clean' with each use.  Setup script could install the RPM repo and other pre-reqs
         * Manual process = bad, this needs to be automated.  Infeasbile to log on to 10+ machines to do all of this   
 # User Documentation Thoughts (Brian)
I think the user documentation should be organized around a set of use cases / sample queries. 
http://psps.perfsonar.net/client-doc.html is a good start in this direction. 
High level:

From all MAs:
 * Get list of all 'metadata', e.g. interfaces, endpoints, etc.
 * Get a list of all 'metadata' but filter by something, e.g. domain, ip range, specific characteristic (1G interfaces, or dual homed hosts)
 * Get all 'data' for a given 'metadata' (by key, filtered by type)
 * Get all 'data' for a given 'metadata' for a given time range or via a statistical function (e.g. averaged).

From an hLS:
 * List of all registered services
 * List of services filtered by service elements (e.g. name, location)
 * List of services filtered by contained metadata items (see MA queries)
 * List of metadata for a given service
 * List of metadata for a given service filtered by data type (see MA queries)

Here are some detailed examples that I think are helpful:
 * SNMP MA:
   *  give me a list of all router interfaces
   *  give me a list of all router interfaces at FNAL
   *  give me the last hour average utilization for interface X
   *  give me that last hour maximum utilization for interface X
   *  give me average and max utilization data for June of last year

  * pSB MA:
    *  give me the list of src/dest pairs stored in this MA
    *  give me all bwctl results for the past 24 hours from host A to host B
    * give me all owamp results for the past 24 hours from host A to host B
    *  give me all bwctl and owamp results from June of last year

 * LS:
   *  give me a list of all bwctl servers listed under project "ESnet" or "DOE-SC-LAB"
   *  give me a list of all ESnet pSB MAs
   *  give me a list of all SNMP MAs

  * PingER MA:
    * give me a list of pingER data for host A to host B for the past 24 hours

  * Topology MA:
    * add sample here


 # TBD 2
 # TBD 3
 # TBD 4 
 # TBD 5 

== ACTIONS ==

  * _*ACTION*_: