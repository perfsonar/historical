#summary Developers Meeting at Winter 2010 Joint Techs - Salt Lake City, UT

=  Jan 31 2010 Meeting =

== Where ==

Collegiate room (2nd Floor - right next to the main ballroom) in the [http://www.union.utah.edu A. Ray Olpin University Union].  

== When ==

January 31, 2010.
1:00pm - 5:00pm MDT

== Agenda/Minutes ==

 # Attendees: 
   * TBD
 # LiveCD Topics (*Jason*, *Aaron*)
   * Remaining 3.1.X releases
     * Targeting 2 - 3 more this year.  May need to worry about some next year depending on support...
       * 3.1.3 in April (pre MM)
       * 3.1.4 in July (pre JTs)
       * 3.1.5 in Nov/Dec (if needed - hard with SC)
     * Critical bug fixes and software upgrades only
     * Will release faster if something *very* critical comes up
     * [http://lists.debian.org/debian-security-announce/2010/msg00010.html Debian security support for v. 4.0 (etch) is being EOL'ed on Feb 15th]
       * Upgrade?
       * Manually backport?
   * Support Structure
     * Need explicit language in a public area
     * Ex:
       * _We monitor X, Y, and Z for vulnderabilities_
       * _We are will support 3.1 releases through X date_
       * _We will make announcements on vulnerabilties within X days_
       * _We will have fixes available for vulnerabilities within X days_
     * SLAC Kernel Discussion
       * SLAC's policy: follow the RHEL kernels.  
       * Our (_unstated right now_) policy: Choose the [http://lkml.org/lkml/2008/10/11/235 long term support kernel] and follow the releases here.  
       * Setting a policy matters to the VOs (e.g. USATLAS).  Choosing a specific policy matters more to individual stake holders. We should weigh the options of following a packaged kernel linage vs what the support structure of the OSS community has to offer.  
   * RHEL (3.2) Transition
     * Targeting Summer for 3.2 Betas
     * Must support 3.1 in parallel
     * VOs may be slow to adopt 3.2 - they are still not recommending 3.1 to Tier3s for example.  
     * Features to worry about
       * Wizard interfaces
       * Backend magic to manage the disk
       * Upgrade path?
       * RPMs for all services and packages
       * Kernels (see above) - and if we want to support web100 long term
       * Install to disk option
 # Nagios integration (*Jason*, *Brian*)
   * Worth experimenting with basic installation/configuration of NAGIOS on current generation?
   * Configuration
     * Static - easy stuff
       * Certain processes/data sets should always be monitored
         * httpd
         * ntpd
         * service watcher?       
         * Disk levels - go below a threshold
         * Load Levels - go above a threshold
         * Process Count?
           * Total goes above some threshold
           * Too many of one type, e.g. too many owampd's or bwctld's (indicates people testing to you)
       * Alert intervals (how often to send alerts for the same outstanding event)
     * Dynamic - the hard stuff
       * Email addresse(s) to send alerts to (GUI tie in)
       * Proccesses/data to monitor 
         * ssh (if enabled)
         * owampd/bwctld/npad/ndt/pS daemons/pow(master|collector)/bw(master|collector) (if enabled)
         * mysqld (if enabled)      
         * data sets from mysql (owamp/bwctl/pinger where applicable)
           * if data is above/below (too many errors or too little bandwidth) a performance threshold
           * if data is older than a time threshold           
         * data sets from rrd (where applicable)
           * if data is above/below a performance threshold (too many errors, too much or too little utilization)
           * if data is older than a time threshold
     * GUI extensions to watch other instances of the toolkit in the wild [requires nrpe probably] (future enhancement)
     * Send a 'come online' alert to a pS address?
 # Testing Platforms (*Eric*, *Jason*, *Brian*)
   * Group decission - what are we supporting for platform and arch?
     * Post policy to various web sites (psps.internet2, software.internet2, others?)
     * re-tool the yum repo (e.g. have a centos 5 vs a centos 4 vs a scientific, etc.) for new packages
     * list will need to flow with the what the vendors and major science VOs are using
     * list also related to our availability to create/test
   * Packages
     * RPM build info in svn currently
     * Deb packages?
     * Each developer responsbile for mainitaing the spec file
     * Release team should be handling the final build and test (currently each developer does this)
     * Release team will handle the uploading and maintenance of the yum repo
   * Buidling and Testing
     * [https://nmi.cs.wisc.edu/ | NMI] - worth using?
     * VM farm 
       * Goal: create scripts that will build/test installation/test use on all arch and platforms in a single swoop
       * Need VMs for each arch and platform, preferably 'clean' with each use.  Setup script could install the RPM repo and other pre-reqs
       * Manual process = bad, this needs to be automated.  Infeasbile to log on to 10+ machines to do all of this      
 # TBD 1
 # TBD 2
 # TBD 3
 # TBD 4 
 # TBD 5 

== ACTIONS ==

  * _*ACTION*_:


