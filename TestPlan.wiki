= perfSONAR-PS release Test Plans =

 * [TestPlan#Introduction Introduction]
 * [TestPlan#Software_Testing_Principals Software Testing Principals]
 * [TestPlan#Current_Status Current Status]
   * [TestPlan#Addressed_Issues Addressed Issues]
   * [TestPlan#Open_Issues Open Issues]
 * [TestPlan#Future_Plans Future Plans]
   * [TestPlan#What_to_Test_and_How_to_Test_it What to Test and How to Test it]
     * [TestPlan#Unit_Testing Unit Testing]
     * [TestPlan#Integration_Testing Integration Testing]
     * [TestPlan#System_Testing System Testing]
     * [TestPlan#System_Integration_Testing System Integration Testing]
   * [TestPlan#Appropriate_Testing_Times Appropriate Testing Times]
     * [TestPlan#Alpha_Testing Alpha Testing]
     * [TestPlan#Beta_Testing Beta Testing]
     * [TestPlan#Regression_Testing Regression Testing]
   * [TestPlan#Service_Specific Service Specific Testing]
     * [TestPlan#LS Lookup Service]
     * [TestPlan#MA Measurement Archive Service]
   * [TestPlan#Open_Issues Open Issues]
       
== Introduction ==

The following write up discusses some general theories and practices regarding
software testing and recommends some actions specifically within the
perfSONAR-PS framework.  All agreed upon actions will become an integral part of
the release management process and would have well defined boundaries and
actors.

Overall the perfSONAR-PS consortium desires testing that accomplishes the
following goals:

 * *Functional Testing*
   * _Unit Level_ - Tests expected vs observed results for each method/module
   * _System Level_ - Tests expected vs observed results for various message types
 * *Performance Testing*
   * _Baseline_ - Establish metrics for acceptable operation of each service and functionality
   * _Regression_ - Regular testing to improve the overall performance of the framework
 * *Scalability* - Ensure that performance degradation is reasonable as number of queries and size of results vary

== Software Testing Principals ==

A full discussion on the nature of software testing, including terminology, is available in the [http://code.google.com/p/perfsonar-ps/wiki/ReleaseManagement release mangagement] discussion.

== Current Status ==

The following represents the current state of testing in the perfSONAR Framework:

 * Unit Testing
   * Many of the modules contain test files written in the common language of perl tests (e.g. [http://perldoc.perl.org/Test/Simple.html Test::Simple], [http://perldoc.perl.org/Test/More.html Test::More]), many include nothing.
     * Each test file should test the methods of the module.
     * Some test files test the interface of the module (see _*Integration Testing*_)
     * These tests are written by the developers in most cases (e.g. _white box_ style - testing based on knowledge of the code).  Some may contain elements of _black box_ testing when dealing with inputs
     * These test cases are executed dynamically as a part of installation
 * Integration Testing
   * With the exception of several APIs, there is no integration testing present
   * *_Unit Testing_* principals that use the perl test language may be used here.
   * The *_test_harness_* may be modified to test interface level communication as well 
 * System Testing
   * The _*test_harness*_ framework was meant to test components from the outside via xml messages.  
     * The messages are written by the developers in most cases (e.g. _white box_ style - testing based on knowledge of the code).  Some may contain elements of _black box_ testing when dealing with message construction
     * These test cases are executed dynamically before a release or during development
   * Profiling is performed using automated [http://search.cpan.org/~samtregar/Devel-Profiler-0.04/lib/Devel/Profiler.pm tools].
     * Profiling addresses non-functional requirements, but to date is not a regular aspect of testing
 * System Integration Testing
   * Small scale testing is done when dealing with external components (e.g. versions of libraries, specific external tools such as databases).  
   * Testing within the confines of the daemon (e.g. deploying several components at once) is tested before a release as well.  
   * We are lacking a way to test scalability in a distributed environment
   
=== Addressed Issues ===   

 * The issue of unit testing has an acceptable solution: using the language of perl testing.  This solution *_must_* be enforced for all checked in code as a function of the release management process
 * The *_test_harness_* is a useful tool for evaluating at the *_System Testing_* level.  This solution also *_must_* be enforced for all services.  Additional development on the robustness of this service would be useful.
 * Use of profile tools (specifically exploring new options such as [http://search.cpan.org/~timb/Devel-NYTProf-2.07/ Devel::NYTProf]) must be executed regularly.  
   
=== Open Issues ===   

 * Currently, no adequate way to address scalability of services
   * Particularly hard problem for TS, hLS, gLS
   * Minor Problem for MAs (will become more evident with GUI development)
 * Enforcement of current methods is lacking (see [TestPlan#Addressed_Issues Addressed Issues])
 * Static analysis (e.g. Code tours and reviews) is absent - will become a part of release process
 * No real analysis on _baseline_ performance available - should test against other perfSONAR tools
 * Design documents outlining original requirements are sparse
 * Service documentation is sparse
 * Documents outlining test recommendations were not present (until now!)

== Future Plans ==

The following represents proposed guidelines for testing in the perfSONAR-PS
framework.  

=== What to Test and How to Test it ===

The following sections are broken down by _size_ of the software unit to be
tested.  Within each section, recommendations on how to test each will be made.

==== Unit Testing ==== 

 # Each Module _must_ contain a test file written to use the perl test language and placed in the appropriate _t/_ location
   # Each test file _must_ test each function _at least_ once (N.B. tests are cheap - write many)
     # Each test case _should_ borrow ideas from _*Equivalence partitioning*_ and _*Boundary value analysis*_ with regards to input cases.  Use of other _black box_ techniques such as _*Decision table testing*_, *Pairwise testing*_, etc. is optional and should be used at the will of the developer to test individual functions
     # Each test case _should_ borrow concepts that test _*Code coverage*_.  Use of other _White box_ techniques are optional.
   # When applicable, each test file _should_ contain hooks that time the various aspects of module and method tests.  These _should_ be recorded and _may_ be used in _Performance_ analysis.
   # Each function and module _should_ be compared to existing design documentation for verification purposes
 # Static reviews of each module are required
   # Each module _must_ be reviewed as a part of the release process for syntactic and stylistic adherence
   # Each module _must_ contain perl documentation for each function, and the module as a whole
   # Each module _should_ contain a _code tour_ (e.g. narrative) of the source code - this can be a separate document or on the wiki
   # Each module _should_ be paired with any related design documentation
     # Design specifications
     # Use cases 

==== Integration Testing ==== 

 # Each *API* and *Module Interface* _must_ contain a way to test it: this may be written in perl test language or may feature some other way to validate the functionality (e.g. similar to *_test_harness_*.
   # Each _must_ contain documentation on use (N.B. this can be included in module documentation, but may be better written as user level documentation)
   # Each _should_ be tested from the _black box_ point of view; use _*Equivalence partitioning*_ and _*Boundary value analysis*_ to help with input selection
 # Each *API* and *Module Interface* _must_ be verified against design specifications
 # Each *API* and *Module Interface* _should_ be reviewed _statically_ for code coverage if applicable (e.g. should be covered in _*Unit Testing*_) 
 
==== System Testing ==== 

 # The developer of each service is responsible for providing examples of each message type a service will accept for use in the *_test_harness_*
   # Each message _should_ be replicated to test many possible input situations as possible; use _*Equivalence partitioning*_ and _*Boundary value analysis*_ to help with input selection
   # Each message _should_ be replicated to test many possible scalability related issues (e.g. result size) as possible 
 # The Release Team will perform and record the results of profiling tests with each release
   # The developer _should_ use these results in pre and post release testing to ensure that performance remains in an acceptable range
 # Each developer is responsible for maintaining documentation:
   # Installation instructions
   # FAQ documentation (web based)
 # Each service _must_ be verified against design specifications

==== System Integration Testing ==== 

 # Each developer _should_ maintain documentation on required libraries and external components
 # Each developer is responsible for testing a service in the context of the overall perfSONAR-PS framework pre and post release
 # The release management team is responsible for testing the services on alternative architectures and operating systems
   # The developer _should_ maintain a list of known issues with architectures and operating systems for use by the release management team and users.  

=== Appropriate Testing Times ===

The following sections detail when the various testing phases should be
completed along with annotations of which individuals are responsible.

==== Alpha Testing ==== 

Alpha testing will be conducted in the time frame immediately after an official
release until the next release makes beta candidates available.  This testing
will be done mostly by developers and users who are interested in staying on the
cutting edge. 

==== Beta Testing ==== 

Beta testing is normally limited to release candidates but may also take into
account mature [TestPlan#Alpha_Testing Alpha] releases. This testing should be
primarily be done by members of the release team and external partners (e.g.
potential users).  

Release candidates that are further along in a release may be opened to a wider
audience of the general public in certain situations.  

==== Regression Testing ==== 

This testing will normally be performed by developers outside of a release
cycle, and by members of the release team within a cycle.  Any time a
significant change is made to a function/module/service this testing should be
performed.

=== Service Specific Testing ===   

==== Lookup Service ====   

The Lookup Service is clearly the most difficult to properly test. Complete testing must involve the ability to modify each of the following independently:
 * the gLS to gLS synchronization update rate
 * the hLS to gLS synchronization update rate
 * the MA to hLS update rate
 * the number of MAs
 * the number of gLS's
 * the number of (src/dest/eventTypes) in an MA

===== Recommended assumptions for Testing =====
 * each MA registers with 2 hLS's for redundancy
 * each 'domain' runs 2 gLS's for redundancy
 * number of MA's should scale to around 100 per gLS
 * number of gLS's should scale to around 100  
 * the synchronization rate should tested at the following intervals: 
   ** 10,30,60 min updates

A planet lab experiment that should be created to allow this testing, and the following should be measured for each test configuration:
 * Query time for a single requests
 * Query time for simultaneous requests (5, 10, 50) to a single gLS
 * measure the change in query time when a hLS or gLS fails


==== Measurement Archive ====   

Test to make sure query time is reasonable if MA scales to 1000's of measurements (src/dest/eventType).


=== Open Issues ====   

 * _*Scalability*_
   * For testing distributed information services (hLS/gLS/TS) - use resources like [http://www.planetlab.org PlanetLab]?
   * For testing the capability of single services (MA/hLS) under multiple simultaneous requests.
   * For testing longevity of services (both busy and non-busy)
 * _*Performance*_
   * Must track the results of performance profiles over time
     * Against similar services (e.g. other perfSONAR releases)
     * Against previous versions of the same service
   * Large result sets
   * Large requests
 * _*Functionality*_
   * Force 'faults' to test the error handling of functions/methods/services.

== Last Updated ==

$Id$